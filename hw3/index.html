<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 284A Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 284A: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Yunting Zhao and Katherine Liu</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://katherine-liu0.github.io/hw-webpages-sp24-YuntingZh/hw3/index.html">https://katherine-liu0.github.io/hw-webpages-sp24-YuntingZh/hw3/index.html</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>


<div>

<h2 align="middle">Overview</h2>
<p>
    In this assignment, we created a physically-based renderer via path tracing, focusing on ray-scene intersections, acceleration structures, direct and indirect lighting, and adaptive sampling. We start with basic ray generation and move to more complex lighting models and efficiency optimizations, including BVH for scene traversal and adaptive sampling based on color variance. The goal is to efficiently render realistic images while balancing quality and render times, and to understand rendering principles.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
    In the rendering pipeline, the process begins with the generation of rays. Rays are projected from the virtual camera through each pixel on the image plane. Each ray carries information such as its origin (the camera position) and direction (determined by the pixel coordinates).
These rays are then sent into the scene to check for intersections with objects. When a ray intersects with an object in the scene, the intersection point is calculated along with other relevant information, such as the surface normal and material properties. This intersection information is used to determine the color of the corresponding pixel.

</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
Edge Vectors and Normal Calculation: We start by calculating two edge vectors of the triangle, using its three vertices. The cross product of these vectors gives us a normal vector to the plane in which the triangle lies.
<br><br>Determining Intersection with Plane: Then find if the ray intersects this plane by checking if the ray direction and the normal to the plane are perpendicular. If they are, there is no intersection.
<br><br>Barycentric Coordinates: If an intersection is found, use barycentric coordinates to determine if the intersection point lies within the boundaries of the triangle. This involves solving a set of linear equations to find the barycentric coordinates, which indicate the weight each vertex contributes to the intersection point.
<br><br>Intersection Point and Normal: Once we confirm the ray intersects within the triangle, the exact point of intersection is computed. The normal at this point is found by interpolating the normals of the triangle's vertices, weighted by the barycentric coordinates.
<br><br>Range Check: Finally, we check if the intersection point lies within the valid range of the ray (between min_t and max_t). If it does, the ray is updated with the intersection details.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<img src="images/part1/Part1_some_dae.png" align="middle" width="800px"/>
<br>

<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
	The Bounding Volume Hierarchy (BVH) construction algorithm has 2 steps: initialization and partitioning. During initialization, objects in the scene are set as leaf nodes. The partitioning step involves choosing a splitting plane using the Surface Area Heuristic (SAH) to minimize the total cost, and then constructing the BVH tree. This process is recursively performed until the end conditions are met, establishing a hierarchy of bounding boxes to improve the efficiency of ray intersection computations.
	<br>
	The heuristic chosen for picking the splitting point is the Surface Area Heuristic (SAH) which aims to minimize the total cost of traversing the BVH tree by evaluating the potential costs of different splitting planes. It takes into account the surface area of bounding boxes and the number of primitives (e.g. triangles) in each partition. The goal is to find a balance between the number of primitives in each child node and the likelihood of a ray intersecting the bounding volumes, thereby reducing the number of intersection tests needed during rendering.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<img src="images/part2/Part2.png" align="middle" width="800px"/>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>

<p>
    For several scenes with moderately complex geometries, rendering times with BVH acceleration are significantly shorter than without BVH. (0.08s vs. 29.42s) This is because of the effective organization of objects within the scene by the BVH construction algorithm, which reduces the number of intersection tests required and thus enhances rendering efficiency. BVH acceleration decreases the number of objects that need to be checked during the rendering process, making it more efficient and faster.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
   In our path tracer, uniform hemisphere sampling estimates direct lighting by distributing samples evenly across a hemisphere centered on the surface normal at the intersection point. This approach treats all directions equally, aiming to capture the overall ambient illumination without bias towards specific light sources. Each sample's contribution is weighted by the surface's reflectance properties and the angle between the sample direction and the surface normal, integrating these contributions to approximate the direct lighting. This method, while straightforward and unbiased, may not efficiently capture the contribution from distinct light sources, potentially requiring a higher number of samples for accurate illumination estimation.
<br><br>
In comparison, light source (importance) sampling targets the calculation of direct lighting more efficiently by prioritizing samples towards the directions of actual light sources. For each light in the scene, we sample directions based on the light's position and intensity, checking for visibility from the intersection point to avoid shadows. This technique allows for a more accurate and noise-reduced estimation of direct lighting from discrete lights by concentrating sampling efforts where they are most impactful. The calculated contributions from visible lights are then aggregated, offering a precise representation of direct illumination with potentially fewer samples compared to uniform hemisphere sampling.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part3/1_Uniform Hemisphere Sampling_sky_CBbunny.png" align="middle" width="400px"/>
        <figcaption>Uniform Hemisphere Sampling, bunny</figcaption>
      </td>
      <td>
        <img src="images/part3/1_Light Sampling_sky_CBbunny.png" align="middle" width="400px"/>
        <figcaption>Light Sampling, bunny</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part3/2_Uniform Hemisphere Sampling_sky_CBspheres_lambertian.png" align="middle" width="400px"/>
        <figcaption>Uniform Hemisphere Sampling, spheres</figcaption>
      </td>
      <td>
        <img src="images/part3/2_Light Sampling_sky_CBspheres_lambertian.png" align="middle" width="400px"/>
        <figcaption>Light Sampling, sphere</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part3/l1_s1_CBspheres_lambertian.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/part3/l4_s1_CBspheres_lambertian.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays (spheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part3/l16_s1_CBspheres_lambertian.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/part3/l64_s1_CBspheres_lambertian.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays (spheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    Based on the screenshots, light sampling produces better images compared to uniform hemisphere sampling. In a setting involving soft shadows from area lights, light sampling captures the variations in lighting intensity and shadow softness with fewer samples, therefore reducing noise and improving the quality of the rendered image. 

</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  Uniform hemisphere sampling is straightforward and computationally efficient but can result in higher noise levels, especially in scenes with complex lighting or soft shadows. This method uniformly samples all directions, which may not always contribute significantly to the final lighting at a point, leading to a need for more samples to reduce noise. Light sampling, by contrast, directly targets the light source, potentially offering more accurate and noise-free results for direct lighting calculations, especially in scenes with well-defined light sources. This method can reduce noise in soft shadows because it focuses sampling on areas that contribute most to the lighting. But it might require more complex calculations, such as accounting for the visibility from the point to the light source, which can increase computational costs.

</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
	We begin by assessing if the ray intersects with any scene geometry using a bounding volume hierarchy (BVH) for efficiency. For rays that intersect, we first add the contribution from light sources hitting the object directly (zero-bounce radiance). Then, depending on the ray's depth and the material properties of the intersected object (whether it's a delta material or not), we add the contribution from the first bounce of indirect light (one-bounce radiance). For subsequent bounces, we use a recursive approach where each bounce's contribution is determined by sampling the BSDF to generate a new direction for the ray, adjusting its depth, and invoking the same function to calculate further indirect contributions. This recursion is controlled by a termination condition based on the ray's depth and a probability determined by Russian Roulette for efficiency (coin_flip(0.7)). The result combines direct lighting with multiple bounces of indirect illumination, capturing the complex interplay of light in the scene for a more realistic rendering.




</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example1.dae</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example2.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (bunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (spheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Direct illumination accounts for the light directly from light sources, producing sharp shadows and defining the basic lighting of the scene. In comparison, indirect illumination captures the light that has bounced off surfaces. It's more dim but it fills the scene with soft light and reveals details not visible with only direct light. 
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Adjusting the maximum ray depth parameter influences the illumination effects, shadow details, reflections, refractions, and the balance between rendering speed and quality. Increasing the depth enhances realism by capturing more complex light interactions, but it also increases the computational load and rendering time. 
</p>
<br>
<h3>For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), and isAccumBounces=false.</h3>
<img src="images/part4/Part4_noAccum.png" align="middle" width="800px"/>
<h3>For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, 4, and 5(the -m flag). Use 1024 samples per pixel.</h3>
<img src="images/part4/Part4_Accum.png" align="middle" width="800px"/>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    Increasing the samples per pixel rate improves the rendering quality by reducing aliasing and noise, but it also raises the computational cost and extends the rendering time. Different samples per pixel rates produce noticeable differences in detail, realism, and clarity. Higher rates significantly improve the image quality at the cost of longer rendering times.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    Adaptive sampling dynamically adjusts the sampling rate based on the variation in pixel colors across the image. In the process of rendering an image, generally, a higher sampling rate leads to higher image quality but also increases the computational cost and time required for rendering. But with adaptive sampling, where it is understood that not all parts of an image contribute equally to its perceived quality, areas with little color variation can be sampled at a lower rate, while areas with significant color variation—where details and textures are critical—can be sampled more heavily. This strategy helps ensure image quality where it's needed most while minimizing unnecessary computations in less critical areas, thereby improving overall rendering efficiency.
	<br>we begin with a baseline number of samples per pixel, gradually increasing the count in predetermined batches until reaching the maximum specified samples or the variance within those samples meets our quality threshold. Mathematically, for each pixel, we calculate the mean (μ) and standard deviation (σ) of the accumulated radiance values after each batch of samples. The process leverages the standard deviation to estimate the confidence interval,</p> 
<img src="images/adaptive_equation.png" align="middle" width="200px"/>
	<p>where n is the current number of samples. This interval helps us determine the uncertainty in our mean radiance estimate. If I becomes smaller than a set fraction (maxTolerance×μ), indicative of low variance, we deem the pixel sufficiently sampled. 
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/bunny2048.png" align="middle" width="400px"/>
        <figcaption>Rendered image (bunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny2048_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (bunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part4/spheres2048.png" align="middle" width="400px"/>
        <figcaption>Rendered image (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/spheres2048_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (spheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
