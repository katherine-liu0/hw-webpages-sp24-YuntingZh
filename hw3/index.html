<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <style>
    body {
      background-color: white;
      padding: 100px;
      width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      color: #121212;
    }

    h1,
    h2,
    h3,
    h4 {
      font-family: 'Source Sans Pro', sans-serif;
    }

    kbd {
      color: #121212;
    }
  </style>
  <title>CS 284A Path Tracer</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

</head>


<body>

  <h1 align="middle">CS 284A: Computer Graphics and Imaging, Spring 2024</h1>
  <h1 align="middle">Project 3-1: Path Tracer</h1>
  <h2 align="middle">Yunting Zhao and Katherine Liu</h2>

  <!-- Add Website URL -->
  <h2 align="middle">Website URL: <a
      href="https://katherine-liu0.github.io/hw-webpages-sp24-YuntingZh/hw3/index.html">https://katherine-liu0.github.io/hw-webpages-sp24-YuntingZh/hw3/index.html</a>
  </h2>

  <br><br>


  <div align="center">
    <table style="width=100%">
      <tr>
        <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
    </table>
  </div>


  <div>

    <h2 align="middle">Overview</h2>
    <p>
      In this assignment, we created a physically-based renderer via path tracing, focusing on ray-scene intersections,
      acceleration structures, direct and indirect lighting, and adaptive sampling. We start with basic ray generation
      and move to more complex lighting models and efficiency optimizations, including BVH for scene traversal and
      adaptive sampling based on color variance. The goal is to efficiently render realistic images while balancing
      quality and render times, and to understand rendering principles.
    </p>
    <br>

    <h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
    <!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

    <h3>
      Explain the ray generation and primitive intersection parts of the rendering pipeline.
    </h3>
    <p><strong>Overview:</strong><br><br>
      In the rendering pipeline, the process begins with the generation of rays. Rays are projected from the virtual
      camera through each pixel on the image plane. Each ray carries information such as its origin (the camera
      position) and direction (determined by the pixel coordinates).
      These rays are then sent into the scene to check for intersections with objects. When a ray intersects with an
      object in the scene, the intersection point is calculated along with other relevant information, such as the
      surface normal and material properties. This intersection information is used to determine the color of the
      corresponding pixel.

    </p>
    <p><strong>What we did:</strong></p>
    <p>Ray generation is the first step in the ray tracing process. In this phase, rays are created and cast from the
      viewpoint (camera) into the scene. For each pixel, a ray is generated and projected into the scene. The direction
      of these rays depends on several factors, including the camera's position, orientation, field of view, and the
      aspect ratio of the viewport. These rays are used to determine how each pixel of the final image is colored by
      simulating the interactions of light with objects in the scene.</p>
    <p>That's what we did in Part 1, Task 1 and 2. In the <code>Camera::generate_ray</code> function, we convert image
      coordinates (x, y) to camera space using the lerp function, and then we create a ray based on the origin and
      sensor point position. In the end, we normalized the ray direction and set <code>min_t</code> and
      <code>max_t</code> (near clipping plane and far clipping plane values). Now, we can use the
      <code>generate_ray</code> function in <code>PathTracer::raytrace_pixel</code> to actually generate rays, trace
      them, and calculate the average color to update the pixel!
    </p>

    <p>Primitive intersection is the process of determining whether, and where, a ray intersects with objects in the
      scene. Usually, the scene is composed of geometric primitives (like triangles, spheres, etc.). For each ray, the
      rendering system performs intersection tests with these primitives to find out if the ray hits any of them. If
      there's a hit, the exact point of intersection is calculated. This includes details like the distance from the ray
      origin to the point of intersection and the surface normal at the point of intersection. If a ray doesn't
      intersect with any object, it might either contribute to the background color or to environmental effects like fog
      or sky lighting.</p>

    <p>And in Part 1, Task 3 and 4, we tackled Ray-Triangle Intersection and Ray-Sphere Intersection. For the
      Ray-Triangle Intersection, we used the Möller-Trumbore intersection algorithm. </p>
    <p><strong> Explain the triangle intersection algorithm:</strong></p>
    <!-- <p>
      Edge Vectors and Normal Calculation: We start by calculating two edge vectors of the triangle, using its three
      vertices. The cross product of these vectors gives us a normal vector to the plane in which the triangle lies.
      <br><br>Determining Intersection with Plane: Then find if the ray intersects this plane by checking if the ray
      direction and the normal to the plane are perpendicular. If they are, there is no intersection.
      <br><br>Barycentric Coordinates: If an intersection is found, use barycentric coordinates to determine if the
      intersection point lies within the boundaries of the triangle. This involves solving a set of linear equations to
      find the barycentric coordinates, which indicate the weight each vertex contributes to the intersection point.
      <br><br>Intersection Point and Normal: Once we confirm the ray intersects within the triangle, the exact point of
      intersection is computed. The normal at this point is found by interpolating the normals of the triangle's
      vertices, weighted by the barycentric coordinates.
      <br><br>Range Check: Finally, we check if the intersection point lies within the valid range of the ray (between
      min_t and max_t). If it does, the ray is updated with the intersection details.
    </p> -->


    <p>
      The Moller-Trumbore ray-triangle intersection algorithm is a streamlined approach for determining if a ray
      intersects a triangle in 3D space. It leverages barycentric coordinates to optimize the process, bypassing the
      need to compute the intersection with the plane on which the triangle lies directly. This method is highly
      efficient for ray tracing applications.
    </p>
    <p>
      In this algorithm, a ray <i>R</i> is represented by <i>R(t) = O + tD</i>, where <i>O</i> is the ray origin,
      <i>D</i> is the direction, and <i>t</i> is a scalar. The triangle is defined by its vertices <i>P<sub>1</sub></i>,
      <i>P<sub>2</sub></i>, and <i>P<sub>3</sub></i>. The algorithm solves for <i>t</i>, <i>u</i>, and <i>v</i>
      (barycentric coordinates) where <i>0 ≤ u, v ≤ 1</i> and <i>u + v ≤ 1</i>, ensuring the intersection point lies
      within the triangle.
    </p>
    <p>
      How we implements this algorithm as follows:
    <ul>
      <li><i>e1</i> and <i>e2</i> are edge vectors of the triangle, computed from <i>P<sub>2</sub> - P<sub>1</sub></i>
        and <i>P<sub>3</sub> - P<sub>1</sub></i>.</li>
      <li><i>h</i> is a vector perpendicular to the ray direction <i>D</i> and edge <i>e2</i>, found using the cross
        product.</li>
      <li>The dot product of <i>e1</i> and <i>h</i> helps in checking if the ray is parallel to the triangle plane (no
        intersection).</li>
      <li><i>f</i> is the inverse of this dot product, playing a critical role in computing the barycentric coordinates
        <i>u</i> and <i>v</i>.</li>
      <li><i>u</i> and <i>v</i> are calculated next, and the conditions <i>u &lt; 0.0</i> or <i>u &gt; 1.0</i>, and <i>v
          &lt; 0.0</i> or <i>u + v &gt; 1.0</i> ensure the intersection point lies within the triangle.</li>
      <li>Finally, <i>t</i> is computed to find the intersection point along the ray. If <i>t</i> falls within a
        specific range, it signifies a valid intersection.</li>
    </ul>
    </p>

    <br>
<p><strong> Explain the Sphere-Ray Intersection algorithm:</strong></p>
<p>
  To determine if a ray intersects a sphere, a quadratic equation is derived from the geometric definitions of a ray and a sphere. A ray is described by the equation <i>R(t) = O + tD</i>, where <i>O</i> represents the origin, <i>D</i> is the direction, and <i>t</i> is a scalar. A sphere is defined as the set of all points <i>X</i> that satisfy the equation <i>(X - C)<sup>2</sup> = r<sup>2</sup></i>, where <i>C</i> is the center of the sphere, and <i>r</i> is its radius.
  </p>
  <p>
  The intersection points between the ray and the sphere are determined by substituting the ray equation into the sphere equation and solving for <i>t</i>. This process typically results in a quadratic equation of the form <i>at<sup>2</sup> + bt + c = 0</i>. The discriminant of this equation, which is the part under the square root in the quadratic formula, indicates whether there are real roots (representing intersections) and, if so, how many.
  </p>
  
  <p>
  In task4, our code implements this algorithm in two main functions: <code>has_intersection</code> and <code>intersect</code>. Here's an overview of each:
  </p>
  <ul>
    <li>
      <b>has_intersection:</b> This function checks if a ray intersects with the sphere. It utilizes the helper function <code>test</code>, which calculates the intersection points (<code>t1</code> and <code>t2</code>). The ray intersects with the sphere if either of these points lies between <code>r.min_t</code> and <code>r.max_t</code>, the minimum and maximum bounds of the ray.
    </li>
    <li>
      <b>intersect:</b> This function not only checks for intersection but also updates intersection data. It calculates the exact intersection point and normal at that point. The intersection point is determined by the formula <code>r.o + t * r.d</code>, where <code>t</code> is the smaller of the two intersection points that fall within the ray's bounds. The normal is calculated by subtracting the sphere's origin from the intersection point and normalizing the resulting vector. The function updates the intersection data structure with this information, along with a reference to the sphere and its Bidirectional Scattering Distribution Function (BSDF).
    </li>
  </ul>
  
<br>
    <h3>
      Show images with normal shading for a few small .dae files.
    </h3>
    <img src="images/part1/Part1_some_dae.png" align="middle" width="800px" />
    <br>

    <h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
    <!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

    <h3>
      Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
    </h3>
    <p>
      The Bounding Volume Hierarchy (BVH) construction algorithm has 2 steps: initialization and partitioning. During
      initialization, objects in the scene are set as leaf nodes. The partitioning step involves choosing a splitting
      plane using the Surface Area Heuristic (SAH) to minimize the total cost, and then constructing the BVH tree. This
      process is recursively performed until the end conditions are met, establishing a hierarchy of bounding boxes to
      improve the efficiency of ray intersection computations.
      <br>
      The heuristic chosen for picking the splitting point is the Surface Area Heuristic (SAH) which aims to minimize
      the total cost of traversing the BVH tree by evaluating the potential costs of different splitting planes. It
      takes into account the surface area of bounding boxes and the number of primitives (e.g. triangles) in each
      partition. The goal is to find a balance between the number of primitives in each child node and the likelihood of
      a ray intersecting the bounding volumes, thereby reducing the number of intersection tests needed during
      rendering.
    </p>

    <h3>
      Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
    </h3>
    <img src="images/part2/Part2.png" align="middle" width="800px" />
    <br>

    <h3>
      Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration.
      Present your results in a one-paragraph analysis.
    </h3>

    <p>
      For several scenes with moderately complex geometries, rendering times with BVH acceleration are significantly
      shorter than without BVH. (0.08s vs. 29.42s) This is because of the effective organization of objects within the
      scene by the BVH construction algorithm, which reduces the number of intersection tests required and thus enhances
      rendering efficiency. BVH acceleration decreases the number of objects that need to be checked during the
      rendering process, making it more efficient and faster.
    </p>
    <br>

    <h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
    <!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

    <h3>
      Walk through both implementations of the direct lighting function.
    </h3>
    <p>
      <strong> Understanding uniform hemisphere sampling v.s. light source (importance) sampling</strong><br><br>
      In our path tracer, uniform hemisphere sampling estimates direct lighting by distributing samples evenly across a
      hemisphere centered on the surface normal at the intersection point. This approach treats all directions equally,
      aiming to capture the overall ambient illumination without bias towards specific light sources. Each sample's
      contribution is weighted by the surface's reflectance properties and the angle between the sample direction and
      the surface normal, integrating these contributions to approximate the direct lighting. This method, while
      straightforward and unbiased, may not efficiently capture the contribution from distinct light sources,
      potentially requiring a higher number of samples for accurate illumination estimation.
      <br><br>
      In comparison, light source (importance) sampling targets the calculation of direct lighting more efficiently by
      prioritizing samples towards the directions of actual light sources. For each light in the scene, we sample
      directions based on the light's position and intensity, checking for visibility from the intersection point to
      avoid shadows. This technique allows for a more accurate and noise-reduced estimation of direct lighting from
      discrete lights by concentrating sampling efforts where they are most impactful. The calculated contributions from
      visible lights are then aggregated, offering a precise representation of direct illumination with potentially
      fewer samples compared to uniform hemisphere sampling.
    </p>
    <p>
      <strong> How we implemented Part 3</strong><br><br>
      In Part 3, we started by modifying <code>DiffuseBSDF::f</code> to return <code>reflectance / PI</code>, and
      calculating <code>*pdf = 1.0 / (2 * PI)</code> for uniformly sampling from the hemisphere in
      <code>DiffuseBSDF::sample_f</code>. In Task 2, we implemented <code>PathTracer::zero_bounce_radiance</code>. Here,
      we check if the object's surface is emissive; if so, we return <code>emission = isect.bsdf->get_emission()</code>.
      This allows us to render and visualize the light sources in the scene.
    </p>

    <p>For Hemisphere Sampling, we use the ray and intersection information in a loop that iterates
      <code>num_samples</code> times. Each sample represents a direction over the hemisphere to check for light
      contribution. We generate a sample direction and use it to create a new ray from the intersection point. If this
      new ray intersects with any light source in the scene (<code>bvh->intersect(new_ray, &new_isect)</code>), we
      calculate the contribution of that light source based on the BSDF and the emission of that light source. The light
      contribution is adjusted for the probability density function of the sample, which is <code>1.0 / (2 * PI)</code>,
      and the total light contribution from all samples is averaged and returned.
    </p>

    <p>For Importance Sampling, we have a light loop that iterates over all the lights in the scene. For each light, we
      use <code>ns_area_light</code> samples to estimate the lighting. Within this sampling loop, for each sample from
      each light source, we obtain the sample direction, the <code>pdf</code>, and the distance to the light using the
      <code>sample_L</code> function. We use <code>EPS_F</code> when updating <code>shadow_ray.max_t</code> to avoid
      numerical precision issues. The light contribution is calculated when the shadow ray does not intersect any
      object. Similar to the previous process, we calculate the light contribution for that particular light and then
      divide it by the <code>pdf</code>. When accumulating the light contributions, we divide by
      <code>num_samples</code> and return the final value.
    </p>


    <h3>
      Show some images rendered with both implementations of the direct lighting function.
    </h3>
    <img src="images/part3/Part3_compare_2_direct_lightings.png" align="middle" width="100%" />

    <br>

    <h3>
      Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b>
      when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using
      light sampling.
    </h3>
    <img src="images/part3/Part3_compare_noise_level_different_ray.png" align="middle" width="100%" />


    <p>
      Based on the screenshots, light sampling produces better images compared to uniform hemisphere sampling. In a
      setting involving soft shadows from area lights, light sampling captures the variations in lighting intensity and
      shadow softness with fewer samples, therefore reducing noise and improving the quality of the rendered image.

    </p>
    <br>

    <h3>
      Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
    </h3>
    <p>
      Uniform hemisphere sampling is straightforward and computationally efficient but can result in higher noise
      levels, especially in scenes with complex lighting or soft shadows. This method uniformly samples all directions,
      which may not always contribute significantly to the final lighting at a point, leading to a need for more samples
      to reduce noise. Light sampling, by contrast, directly targets the light source, potentially offering more
      accurate and noise-free results for direct lighting calculations, especially in scenes with well-defined light
      sources. This method can reduce noise in soft shadows because it focuses sampling on areas that contribute most to
      the lighting. But it might require more complex calculations, such as accounting for the visibility from the point
      to the light source, which can increase computational costs.

    </p>
    <br>


    <h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
    <!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

    <h3>
      Walk through your implementation of the indirect lighting function.
    </h3>
    <p>

      For Part 4, we started by implementing <code>DiffuseBSDF::sample_f </code>to represent a diffuse material, instead
      of using <code>*pdf = 1.0 / (2 * PI);</code> we used <code>*pdf = cosTheta / PI</code> to calculate the
      <code>pdf</code>for sampled direction. Then, we
      implemented <code>at_least_one_bounce_radiance</code>. We begin by checking if the maximum depth is reached, and
      if so, return
      black. Then to ensure that all paths have at least one calculation of direct illumination before reaching the
      maximum depth, we will return <code>one_bounce_radiance</code> when the recursive depth of the ray reaches
      <code>max_ray_depth - 1</code>.

      Then we calculate the sampling direction and the new ray, and use the intersection of this new ray with the scene
      as a criterion to decide whether to perform a recursive<code>L_out</code> , divided by<code>pdf</code> . Finally,
      we return the value of
      <code> L_one + L_out</code>. In the implementation of Russian Roulette, before calculating <code>L_out</code>, we
      decide whether to track
      based on<code> cpdf</code>. If we do proceed, the value of<code> L_out</code> is then divided by
      <code> cpdf</code>to ensure no bias.
    </p>
    <br>

    <h3>
      Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
    </h3>
    <img src="images/part4/Part4_some_imgs.png" align="middle" width="800px" />
    <p>
      We tested out many imgs but only CBspheres_lambertian and CBbunny has BSDF material so all of the others are just
      black when I run it.
    </p>

    <br>

    <h3>
      Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination.
      Use 1024 samples per pixel.
    </h3>
    <img src="images/part4/Part4_directVSindirect.png" align="middle" width="100%" />
    <img src="images/part4/Part4_howtoOnlyIndriect.png" align="middle" width="100%" />
    <p>
      Through the comparison, we can see the difference between direct and indirect lighting. For the indirect lighting,
      I modified my function to exclude the zero-bounce radiance and include only at_least_one_bounce_radiance, starting
      from the second bounce (by removing L_one). Strictly speaking, "only direct illumination" or "only indirect
      illumination" does not include zero-bounce ambient illumination because it does not involve a direct path from the
      light source to the object. However, in practical rendering scenarios, especially in simpler or real-time
      renderings, ambient light (or a similar concept) is often included alongside direct illumination to provide a
      baseline level of light in the scene. Just out of curiosity, I also rendered an image up there that includes the
      zero-bounce radiance with my indirect lighting. It's only 64 samples, but it can demonstrate the idea.</p>

    <br>
    <p>
      Direct illumination accounts for the light directly from light sources, producing sharp shadows and defining the
      basic lighting of the scene. In comparison, indirect illumination captures the light that has bounced off
      surfaces. It's more dim but it fills the scene with soft light and reveals details not visible with only direct
      light.
    </p>
    <br>
    <h3>For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag),
      and isAccumBounces=false.</h3>
    <img src="images/part4/Part4_noAccum.png" align="middle" width="100%" />
    <h3>For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, 4, and 5(the -m flag). Use 1024
      samples per pixel.</h3>
    <img src="images/part4/Part4_Accum.png" align="middle" width="100%" />
    <p>
    <ul>
      <li>max_ray_depth = 0:

        This setting effectively means no light bounces are considered. The rendered image will show only the
        zero-bounce radiance (ambient lighting) without any direct or indirect illumination. Objects won't have shading
        or shadows.</li>
      <li>max_ray_depth = 1:

        Only direct illumination is considered. Light travels from the source to the object without any bounces. We can
        see direct lighting effects like bright spots and sharp shadows, but no indirect illumination effects like
        reflections or color bleeding.</li>
      <li>max_ray_depth = 2:

        The first indirect bounce (2nd bounce of light) is now included. In addition to direct lighting, some light
        reflections and subtle color bleeding start to appear. However, the effects are limited as only a single bounce
        of indirect light is considered.</li>
      <li>max_ray_depth = 3:

        The second indirect bounce (3rd bounce of light) is considered. This adds more complexity to the lighting,
        enhancing reflections and color bleeding. The scene starts to look more realistic compared to lower depth
        settings.</li>
      <li>max_ray_depth = 4 and 5:

        With each increase in ray depth, more bounces of light are considered, progressively adding more subtlety and
        realism to indirect lighting effects. The scene appears more naturally lit, with complex interplays of light,
        shadows, and colors.</li>
    </ul>
    </p>
    <br>
    <p><strong>Differences Between 2nd and 3rd Bounces:</strong><br>

      2nd Bounce: Introduces the first level of indirect illumination. Light that hits a surface and then reflects onto
      another surface is captured. This bounce starts to fill in the shadows and adds a bit of reflected color and light
      to the scene, but it's still somewhat simplistic.
      <br>3rd Bounce: Adds another layer of complexity. Light has now interacted with two surfaces before reaching the
      eye
      or camera. This bounce enhances realism by capturing more nuanced interactions between light and the environment,
      such as softer shadows, more complex reflections, and subtle color interplays.
    </p>
    <p><strong>Quality Compared to Rasterization:</strong><br>
      <img src="images/part4/Part4_pathTracing_vs_others.png" align="middle" width="500px" />
    <figcaption>
      I found this image from
      <a href="https://blogs.nvidia.com/blog/direct-indirect-lighting/" target="_blank">NVIDIA's blog will help
        demonstrate the difference between different method</a>.
    </figcaption>
    <ul>
      <li>Realism: Path tracing with multiple bounces captures light interactions more accurately than rasterization,
        leading to more photorealistic images.</li>
      <li>Computational Cost: Each additional bounce increases computational complexity. Rasterization, on the other
        hand,
        is generally faster but less capable of simulating complex light interactions.</li>
      <li>Shadows and Reflections: Path tracing naturally produces soft shadows and realistic reflections, which are
        challenging to achieve with traditional rasterization without additional techniques.</li>
      <li>Direct vs. Indirect Lighting: Rasterization handles direct lighting well but often requires additional methods
        (like global illumination algorithms) for indirect lighting. Path tracing inherently calculates both, creating a
        more unified and realistic lighting model.</li>
    </ul>
    </p>
    <h3>
      For CBbunny.dae, output the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4, and 100(the -m
      flag). Use 1024 samples per pixel.
    </h3>
    <img src="images/part4/Part4_Russian_Roulette.png" align="middle" width="100%" />
    <p>
      Adjusting the maximum ray depth parameter influences the illumination effects, shadow details, reflections,
      refractions, and the balance between rendering speed and quality. Increasing the depth enhances realism by
      capturing more complex light interactions, but it also increases the computational load and rendering time.
    </p>
    <br>

    <h3>
      Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16,
      64, and 1024. Use 4 light rays.
    </h3>
    <img src="images/part4/Part4_compare_different_sample-per-pixel_rates.png" align="middle" width="100%" />

    <p>
      Our observation:<br>
    <ul>
      <li> Noise Reduction: As the sample rate increases, the noise in the image decreases significantly. This is
        because more samples allow for a better average calculation of pixel values, leading to smoother gradients and
        more accurate color representation.</li>

      <li>Render Time: Higher sample rates significantly increase render times. This is because the path tracer needs to
        calculate many more light paths for each pixel, which requires more computational power and time.</li>
      <li>Diminishing Returns: Beyond a certain point, increasing the number of samples per pixel yields diminishing
        improvements in image quality relative to the increase in render time.
      </li>
      In summary, increasing the samples per pixel in a path tracing render leads to progressively better image quality,
      with diminishing noise and more accurate and realistic lighting and details. However, this comes at the cost of
      longer render times, so it's a balance between desired quality and available time/resources.

    </ul>

    </p>
    <p>
      Increasing the samples per pixel rate improves the rendering quality by reducing aliasing and noise, but it also
      raises the computational cost and extends the rendering time. Different samples per pixel rates produce noticeable
      differences in detail, realism, and clarity. Higher rates significantly improve the image quality at the cost of
      longer rendering times.
    </p>
    <br>


    <h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
    <!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

    <h3>
      Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
    </h3>
    <p>
      Adaptive sampling dynamically adjusts the sampling rate based on the variation in pixel colors across the image.
      In the process of rendering an image, generally, a higher sampling rate leads to higher image quality but also
      increases the computational cost and time required for rendering. But with adaptive sampling, where it is
      understood that not all parts of an image contribute equally to its perceived quality, areas with little color
      variation can be sampled at a lower rate, while areas with significant color variation—where details and textures
      are critical—can be sampled more heavily. This strategy helps ensure image quality where it's needed most while
      minimizing unnecessary computations in less critical areas, thereby improving overall rendering efficiency.
      <br>we begin with a baseline number of samples per pixel, gradually increasing the count in predetermined batches
      until reaching the maximum specified samples or the variance within those samples meets our quality threshold.
      Mathematically, for each pixel, we calculate the mean (μ) and standard deviation (σ) of the accumulated radiance
      values after each batch of samples. The process leverages the standard deviation to estimate the confidence
      interval,
    </p>
    <img src="images/adaptive_equation.png" align="middle" width="200px" />
    <p>where n is the current number of samples. This interval helps us determine the uncertainty in our mean radiance
      estimate. If I becomes smaller than a set fraction (maxTolerance×μ), indicative of low variance, we deem the pixel
      sufficiently sampled.
    </p>
    <br>

    <h3>
      Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly
      visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which
      shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your
      noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part4/bunny2048.png" align="middle" width="400px" />
            <figcaption>Rendered image (bunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part4/bunny2048_rate.png" align="middle" width="400px" />
            <figcaption>Sample rate image (bunny.dae)</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part4/spheres2048.png" align="middle" width="400px" />
            <figcaption>Rendered image (spheres.dae)</figcaption>
          </td>
          <td>
            <img src="images/part4/spheres2048_rate.png" align="middle" width="400px" />
            <figcaption>Sample rate image (spheres.dae)</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>


</body>

</html>